from adaboost import Adaboost
from datetime import datetime, date, timedelta
from joblib import delayed, Parallel, parallel_backend, wrap_non_picklable_objects
import pandas as pd
import numpy as np
import scipy.sparse as sparse
from time import time
from itertools import product
class Task():

    def __init__(self, data_path_lp, data_path_vol, pred_type=None, pred_args=None, assets_for_each_pred=None):
        self.data_path_lp = data_path_lp
        self.data_path_vol = data_path_vol
        self.pred_types = pred_type
        self.pred_args = pred_args
        self.assets_per_pred = assets_for_each_pred
        for i in range(len(self.pred_types)):
            if self.pred_types[i] == "adaboost":
                self.pred_types[i] = Adaboost
            # Add new elifs for new classes here as they are created

    def hyperparams_search(self,parameters, asset, train_window=10000, test_window=1440):
        """Will run hyperparameter search on current prediction types. Will run through all possible combinations of the parameters.
        parameters = dictionary where key=name_of_parameter and value = array of all possible values you want it to take. 
        asset = integer that corresponds to which asset you want to run the hyperparameter search over.
        """
        key_arr = []
        prod_arr = []
        for key in parameters.keys():
            key_arr.append(key)
            prod_arr.append(parameters[key])
        best_corr = -10000
        best_params = {}
        for comb in product(*prod_arr):
            cur_args = {}
            for j in range(len(key_arr)):
                cur_args[key_arr[j]] = comb[j]
            self.pred_args[asset] = cur_args
            regressor_cols = [c for c in self.train_df_assets[0] if c not in ["return", "timestamp"]]
            y = "return"
            corr = self.walkforward_cv_one(self.train_df_assets[asset], y, regressor_cols, train_window, test_window, self.pred_types[asset], self.pred_args[asset])
            print(cur_args)
            print(corr)
            print()
            if corr > best_corr:
                best_corr = corr
                best_params = cur_args
        return (best_corr, cur_args)

    def walkforward_cv(self, train_window=10000, test_window=1440, predictors=None, predictors_arguments=None):
        """Runs walkforward_cv on each model. Returns cv corr."""
        if predictors != None:
            self.pred_types = predictors
            self.pred_args = predictors_arguments
        for i in range(len(predictors)):
            if self.pred_types[i] == "adaboost":
                self.pred_types[i] = Adaboost
        regressor_cols = [c for c in self.train_df_assets[0] if c not in ["return", "timestamp"]]
        y = "return"
        cors = np.zeros(len(self.pred_types))
        for i in range(len(self.pred_types)):
            print(f"running {str(self.pred_types[i])} on asset {i}")
            cors[i] = self.walkforward_cv_one(self.train_df_assets[i], y, regressor_cols, train_window, test_window, self.pred_types[i], self.pred_args[i])
            print(f"walkforward cv gives correlation of {cors[i]}")
        print(cors)
        return cors

    def save_models(self,):
        pass

    def load_models(path_to_models):
        pass

    def train_models(self, predictors=None, predictors_arguments=None, names=None):
        if predictors != None:
            self.pred_types = predictors
            self.pred_args = predictors_arguments
        self.models = []
        for i in range(len(predictors)):
            if self.pred_types[i] == "adaboost":
                self.models.append(Adaboost(self.pred_args[i]))
            regressor_cols = [c for c in self.train_df_assets[i] if c not in ["return", "timestamp"]]
            y = self.train_df_assets[i]["return"]
            self.models[i].fit(self.train_df_assets[i][regressor_cols], y)
        return self.models


    def create_dataframes(self, train_advance=10, minute_lag=30, rsi_k=30):
        """
        Generate dataframe of features to use for prediction
        Params
        ------
        * train_advance: int, number of data points to skip between data points that are kept for training or testing.
            Default is 10.
        * minute_lag: int, number of previous minutes to calculate lagged features over. Default is 30.
        * rsi_k: int, number of previous minutes over which to calculate relative strength index. Default is 30.
        Returns
        -------
        * DataFrame of shape (p * (n / train_advance), j_generated_features + 1) where
        n is nrows of lp and vol, p is number of assets (ncols) in lp and vol, and j_generated_features
        is the number of features generated by the function, which depends on the chosen parameter values
        (the first column of the dataframe, 'return', is the response variable)
        NOTE: the returned dataframe is ordered by asset with increasing timestamp,
        i.e. asset 1 and its features/response variable are the first n / train_advance rows, asset 2 and
        its features/response variable are the next n /train_advance rows, etc.
        """
        print("Making training df")
        lp = pd.read_pickle(self.data_path_lp)
        vol = pd.read_pickle(self.data_path_vol)
        start_time = time()
        full_train_df = pd.DataFrame()
        for j in lp:
            train_df = pd.DataFrame({
                "return": (lp[j].shift(-30) - lp[j])[30::train_advance], # resp variable
                "weekday":  lp.index[30::train_advance].day_of_week.astype(str), # day of the week
            }).dropna()

            log_vol_sum = []
            interval_high = []
            interval_low = []
            rsi = []
            for t in train_df.index:
                log_vol_sum.append(np.log(sum(vol.loc[t - timedelta(minutes=minute_lag):t][j]))) # log(volume sum) over (t - LAG):t
                interval_high.append(max(lp.loc[t - timedelta(minutes=minute_lag):t][j])) # max price over (t - LAG):t
                interval_low.append(min(lp.loc[t - timedelta(minutes=minute_lag):t][j])) # min price over (t - LAG):t
                rsi.append(
                    sum([max(x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]) /
                    (sum([max(x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]) +
                     sum([max(-x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]))) # relative strength index

            train_df["log_vol_sum"] = log_vol_sum
            train_df["interval_high"] = interval_high
            train_df["interval_low"] = interval_low
            train_df["rsi"] = rsi
            train_df["rel_price_range"] = 2 * (train_df["interval_high"] - train_df["interval_low"]) / (
                train_df["interval_high"] + train_df["interval_low"]) # relative price range
            train_df["range_volatility"] = np.sqrt(
                np.square(np.log(np.exp(train_df["interval_high"]) / np.exp(train_df["interval_low"]))) / (4 * np.log(2))
            ) # parkinson's volatility

            for ell in range(1, minute_lag + 1):
                train_df["price_lag_" + str(ell)] = lp[j].shift(ell)[train_df.index] # price at time t - ell
                train_df["vw_price_lag_" + str(ell)] = (lp[j].shift(ell) * vol[j].shift(ell))[train_df.index] # volume-weighted price at time t - ell
                train_df = pd.concat([
                    train_df,
                    lp[[i for i in lp if i != j]].shift(ell).rename(
                        columns={k: "asset_" + str(k) + "_lag_" + str(ell) if k < j
                                 else "asset_" + str(k - 1) + "_lag_" + str(ell)
                                 for k in lp[[i for i in lp if i != j]]}).loc[train_df.index]
                ], axis=1) # price of other assets at time t - ell
            full_train_df = pd.concat([full_train_df, train_df.reset_index()])

        duration = time() - start_time
        print("Finished making training df in %s seconds" % np.round(duration, 4))
        
        full_train_df = full_train_df.reset_index(drop = True)
        self.train_df_assets = []
        print(len(full_train_df))
        for i in range(10):
            self.train_df_assets.append(full_train_df.iloc[int(i*(len(full_train_df)/10)) :int((i+1)*(len(full_train_df)/10)), :])
            print(len(full_train_df.iloc[int(i*(len(full_train_df)/10)) :int((i+1)*(len(full_train_df)/10)), :]))
        return self.train_df_assets


    def walkforward_cv_one(self, data, y, regressor_cols, train_window, test_window, model_class, model_args, parallel=False):
        """
        Run walk-forward cross-validation in parallel for a given model with 'fit' and 'score'
        methods
        NOTE: need to order by increasing timestamp
        Params
        ------
        * data: DataFrame, includes both the response variable and features to use for training
        * y: str, name of the response column in `data`
        * regressor_cols: list, column names to use as features for training
        * train_window: int, number of samples to use for training in each walkforward-window
        * test_window: int, number of samples to use for validation in each walkforward-window
        * model_class: model class to use for training
        * model_args: dict, dictionary of the hyperparameters to use for the model architecture
        * parallel: bool, if True, will run jobs in parallel using a 'multiprocessing' backend. If False,
            will run jobs sequentially
        Returns
        -------
        Correlation coefficient between predictions and true values of the response over all
        of the test windows
        """
        def _fit_and_score(train_X, test_X, model, regressor_cols, y):
            model.fit(train_X[regressor_cols], train_X[y])

            return pd.DataFrame({
                "pred": model.predict(test_X[regressor_cols]),
                "y": test_X[y]})

        @delayed
        @wrap_non_picklable_objects
        def _delayed_fit_and_score(train_X, test_X, model, regressor_cols, y):
            return _fit_and_score(train_X, test_X, model, regressor_cols, y)


        nbatches = int(np.floor((data.shape[0] - train_window) / test_window))
        job_args = {
            b: {
            'train_X': data.iloc[(b * test_window):(b * test_window + train_window)],
            'test_X': data.iloc[(b * test_window + train_window):((b+1) * test_window + train_window)],
            'model': model_class(model_args),
            'regressor_cols': regressor_cols,
            'y': y,
            }
            for b in range(nbatches)
        }
        if parallel:
            model_jobs = [_delayed_fit_and_score(**b) for b in job_args.values()]
            print("Running jobs in parallel")
            with parallel_backend("multiprocessing"):
                out = Parallel(n_jobs=min(nbatches, 20), verbose=11, pre_dispatch='n_jobs')(model_jobs)
            model_scores = pd.concat(out, axis=0)
        else:
            model_scores = pd.DataFrame()
            print("Running jobs sequentially")
            for j in job_args.values():
                model_scores = pd.concat([model_scores, _fit_and_score(**j)], axis=0)

        return np.corrcoef(model_scores, rowvar=False)[0,1]


if __name__ == "__main__":
    t = Task("log_price.df", "volume_usd.df", pred_type=["adaboost" for i in range(10)], pred_args=[{"n_estimators": 10} for i in range(10)])
    a = t.create_dataframes()
    #t.train_models(predictors=["adaboost" for i in range(10)], predictors_arguments=[{"n_estimators": 10} for i in range(10)])
    #t.walkforward_cv(predictors=)
    params = {"n_estimators": [1,50]}
    asset = 0
    print(t.hyperparams_search(params, asset))
