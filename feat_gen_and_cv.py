# Feature generation and walk-through CV code
# Josh Wasserman (jwasserman2)
# April 2022

from datetime import datetime, date, timedelta
from joblib import delayed, Parallel, parallel_backend, wrap_non_picklable_objects
import pandas as pd
import numpy as np
import scipy.sparse as sparse
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import RidgeCV
from time import time
from tslearn.clustering import TimeSeriesKMeans

lp = pd.read_pickle("log_price.df")
vol = pd.read_pickle("volume_usd.df")


def create_features(lp, vol, train_advance=10, minute_lag=30, rsi_k=30):
    """
    Generate dataframe of features to use for prediction

    Params
    ------
    * lp: DataFrame, log price data
    * vol: DataFrame, volume data
    * train_advance: int, number of data points to skip between data points that are kept for training or testing.
        Default is 10.
    * minute_lag: int, number of previous minutes to calculate lagged features over. Default is 30.
    * rsi_k: int, number of previous minutes over which to calculate relative strength index. Default is 30.

    Returns
    -------
    * DataFrame of shape (p * (n / train_advance), j_generated_features + 1) where
    n is nrows of lp and vol, p is number of assets (ncols) in lp and vol, and j_generated_features
    is the number of features generated by the function, which depends on the chosen parameter values
    (the first column of the dataframe, 'return', is the response variable)

    NOTE: the returned dataframe is ordered by asset with increasing timestamp,
    i.e. asset 1 and its features/response variable are the first n / train_advance rows, asset 2 and
    its features/response variable are the next n /train_advance rows, etc.
    """
    print("Making training df")
    start_time = time()
    full_train_df = pd.DataFrame()
    for j in lp:
        train_df = pd.DataFrame({
            "return": (lp[j].shift(-30) - lp[j])[30::train_advance], # resp variable
            "weekday":  lp.index[30::train_advance].day_of_week.astype(str), # day of the week
        }).dropna()

        log_vol_sum = []
        interval_high = []
        interval_low = []
        rsi = []
        for t in train_df.index:
            log_vol_sum.append(np.log(sum(vol.loc[t - timedelta(minutes=minute_lag):t][j]))) # log(volume sum) over (t - LAG):t
            interval_high.append(max(lp.loc[t - timedelta(minutes=minute_lag):t][j])) # max price over (t - LAG):t
            interval_low.append(min(lp.loc[t - timedelta(minutes=minute_lag):t][j])) # min price over (t - LAG):t
            rsi.append(
                sum([max(x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]) /
                (sum([max(x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]) +
                 sum([max(-x, 0) for x in lp.loc[t - timedelta(minutes=rsi_k):t][j]]))) # relative strength index

        train_df["log_vol_sum"] = log_vol_sum
        train_df["interval_high"] = interval_high
        train_df["interval_low"] = interval_low
        train_df["rsi"] = rsi
        train_df["rel_price_range"] = 2 * (train_df["interval_high"] - train_df["interval_low"]) / (
            train_df["interval_high"] + train_df["interval_low"]) # relative price range
        train_df["range_volatility"] = np.sqrt(
            np.square(np.log(np.exp(train_df["interval_high"]) / np.exp(train_df["interval_low"]))) / (4 * np.log(2))
        ) # parkinson's volatility

        for ell in range(1, minute_lag + 1):
            train_df["price_lag_" + str(ell)] = lp[j].shift(ell)[train_df.index] # price at time t - ell
            train_df["vw_price_lag_" + str(ell)] = (lp[j].shift(ell) * vol[j].shift(ell))[train_df.index] # volume-weighted price at time t - ell
            train_df = pd.concat([
                train_df,
                lp[[i for i in lp if i != j]].shift(ell).rename(
                    columns={k: "asset_" + str(k) + "_lag_" + str(ell) if k < j
                             else "asset_" + str(k - 1) + "_lag_" + str(ell)
                             for k in lp[[i for i in lp if i != j]]}).loc[train_df.index]
            ], axis=1) # price of other assets at time t - ell
        full_train_df = pd.concat([full_train_df, train_df.reset_index()])

    duration = time() - start_time
    print("Finished making training df in %s seconds" % np.round(duration, 4))

    return full_train_df.reset_index(drop = True)


def walkforward_cv(data,
                   y,
                   regressor_cols,
                   train_window,
                   test_window,
                   model_class,
                   model_args,
                   parallel=False):
    """
    Run walk-forward cross-validation in parallel for a given model with 'fit' and 'score'
    methods
    NOTE: need to order by increasing timestamp
    Params
    ------
    * data: DataFrame, includes both the response variable and features to use for training
    * y: str, name of the response column in `data`
    * regressor_cols: list, column names to use as features for training
    * train_window: int, number of samples to use for training in each walkforward-window
    * test_window: int, number of samples to use for validation in each walkforward-window
    * model_class: model class to use for training
    * model_args: dict, dictionary of the hyperparameters to use for the model architecture
    * parallel: bool, if True, will run jobs in parallel using a 'multiprocessing' backend. If False,
        will run jobs sequentially
    Returns
    -------
    Correlation coefficient between predictions and true values of the response over all
    of the test windows
    """
    def _fit_and_score(train_X, test_X, model, regressor_cols, y):
        model.fit(train_X[regressor_cols], train_X[y])

        return pd.DataFrame({
            "pred": model.predict(test_X[regressor_cols]),
            "y": test_X[y]})

    @delayed
    @wrap_non_picklable_objects
    def _delayed_fit_and_score(train_X, test_X, model, regressor_cols, y):
        return _fit_and_score(train_X, test_X, model, regressor_cols, y)


    nbatches = int(np.floor((data.shape[0] - train_window) / test_window))
    job_args = {
        b: {
        'train_X': data.iloc[(b * test_window):(b * test_window + train_window)],
        'test_X': data.iloc[(b * test_window + train_window):((b+1) * test_window + train_window)],
        'model': model_class(**model_args),
        'regressor_cols': regressor_cols,
        'y': y,
        }
        for b in range(nbatches)
    }
    if parallel:
        model_jobs = [_delayed_fit_and_score(**b) for b in job_args.values()]
        print("Running jobs in parallel")
        with parallel_backend("multiprocessing"):
            out = Parallel(n_jobs=min(nbatches, 20), verbose=11, pre_dispatch='n_jobs')(model_jobs)
        model_scores = pd.concat(out, axis=0)
    else:
        model_scores = pd.DataFrame()
        print("Running jobs sequentially")
        for j in job_args.values():
            model_scores = pd.concat([model_scores, _fit_and_score(**j)], axis=0)

    return np.corrcoef(model_scores, rowvar=False)[0,1]


def random_kmer_cv(data,
                   y,
                   regressor_cols,
                   kmer_length,
                   prediction_length,
                   model_class,
                   model_args,
                   parallel=False):
    def _fit_and_score(train_X, test_X, model, regressor_cols, y):
        model.fit(train_X[regressor_cols], train_X[y])

        return np.corrcoef(model.predict(test_X[regressor_cols]), test_X[y])[0,1]

    @delayed
    @wrap_non_picklable_objects
    def _delayed_fit_and_score(train_X, test_X, model, regressor_cols, y):
        return _fit_and_score(train_X, test_X, model, regressor_cols, y)


    start = np.random.randint(low=0, high=kmer_length + prediction_length)
    batch_starts = []
    job_args = {
        b: {
        'train_X': data.iloc[(b * test_window):(b * test_window + train_window)],
        'test_X': data.iloc[(b * test_window + train_window):((b+1) * test_window + train_window)],
        'model': model_class(**model_args),
        'regressor_cols': regressor_cols,
        'y': y,
        }
        for b in range(nbatches)
    }
    if parallel:
        model_jobs = [_delayed_fit_and_score(**b) for b in job_args.values()]
        print("Running jobs in parallel")
        with parallel_backend("multiprocessing"):
            model_scores = Parallel(n_jobs=min(nbatches, 20), verbose=11, pre_dispatch='n_jobs')(model_jobs)
    else:
        model_scores = []
        print("Running jobs sequentially")
        for j in job_args.values():
            model_scores.append(_fit_and_score(**j))

    return model_scores


train_df = create_features(lp.iloc[0:10000], vol.iloc[0:10000])
train_df = train_df.sort_values("timestamp").dropna()
regressor_cols = [c for c in train_df.columns if c not in ["return", "timestamp"]]
model_scores = walkforward_cv(train_df, "return", regressor_cols, 2000, 200, RidgeCV,
               {"alphas": np.logspace(-1, 1)}, parallel=True)
np.mean(model_scores)

# correlation among asset returns
returns = (lp.shift(-30) - lp).dropna()
returns_corr = np.corrcoef(returns, rowvar=False)
returns_corr_df = pd.DataFrame(np.vstack((np.repeat(np.arange(10), 10), np.array([c for c in range(10)] * 10))).T)
returns_corr_df["corr"] = 0
for x, y in zip(returns_corr_df[0], returns_corr_df[1]):
    returns_corr_df.loc[(returns_corr_df[0] == x) & (returns_corr_df[1] == y), "corr"] = returns_corr[x, y]
plt.scatter(returns_corr_df[0], returns_corr_df[1], c=np.sign(returns_corr_df["corr"]),
            s=np.abs(returns_corr_df["corr"]) * 1000, cmap="RdBu")

# asset 2 modeling
minute_lag = 30
asset_2_df = pd.DataFrame({"return": returns.loc[returns.index[30::10], 2]})
asset_2_df = pd.concat([
    asset_2_df,
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].max(0) for idx in asset_2_df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_interval_high" for k in range(10)}).set_index(asset_2_df.index),
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].min(0) for idx in asset_2_df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_interval_low" for k in range(10)}).set_index(asset_2_df.index),
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(x, 0)).sum(0) /
        (lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(x, 0)).sum(0) +
         lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(-x, 0)).sum(0))
        for idx in asset_2_df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_rsi" for k in range(10)}).set_index(asset_2_df.index)],
    axis=1)
for asset in np.arange(10):
    range_col = "asset_" + str(asset) + "_rel_price_range"
    volatility_col = "asset_" + str(asset) + "_range_volatility"
    interval_high_col = "asset_" + str(asset) + "_interval_high"
    interval_low_col = "asset_" + str(asset) + "_interval_low"
    asset_2_df[range_col] = 2 * (asset_2_df[interval_high_col] - asset_2_df[interval_low_col]) / (
            asset_2_df[interval_high_col] + asset_2_df[interval_low_col])
    asset_2_df[volatility_col] = np.sqrt(
        np.square(np.log(np.exp(asset_2_df[interval_high_col]) / np.exp(asset_2_df[interval_low_col]))) / (4 * np.log(2))
    )

regressor_cols = [c for c in asset_2_df if c != "return"]
y = asset_2_df["return"]
asset_2_ridge = RidgeCV(alphas=np.logspace(-1, 1))
asset_2_ridge.fit(asset_2_df[regressor_cols], y)
ridge_coefs = pd.DataFrame({"feat": regressor_cols, "coef": asset_2_ridge.coef_}).sort_values("coef")

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
ax.barh(y = ridge_coefs["feat"], width = ridge_coefs["coef"], align = "center")
ax.set_title("Ridge Coeffs")
plt.show()

rf = RandomForestRegressor(n_estimators=100, max_depth=10, max_features=0.5)
rf.fit(asset_2_df[regressor_cols], y)
rf_coefs = pd.DataFrame({"feat": regressor_cols, "feat_importance": rf.feature_importances_}).sort_values("feat_importance")
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
ax.barh(y = rf_coefs["feat"], width = rf_coefs["feat_importance"], align = "center")
ax.set_title("RF(100 trees, max_depth=10, max_features=0.5) Feature Importances")
plt.show()

rf_walkforward_cv_scores = {}
i = 0
for ntrees in [50, 100, 200]:
    for max_depth in [5, 10, 25, 50]:
        for max_features in np.linspace(0.25, 1, 4):
            rf_walkforward_cv_scores[str(i)] = {
                "ntrees": ntrees,
                "max_depth": max_depth,
                "max_features": max_features,
                "oos_corr": walkforward_cv(asset_2_df,
                                            "return",
                                            regressor_cols,
                                            10000,
                                            1440,
                                            RandomForestRegressor,
                                            {"n_estimators": ntrees,
                                             "max_depth": max_depth,
                                             "max_features": max_features,},
                                            parallel=True),
            }
            i +=1



max_oos_corr = max([r['oos_corr'] for r in rf_walkforward_cv_scores.values()])
[v for k, v in rf_walkforward_cv_scores.items() if v['oos_corr'] == max_oos_corr]

# spline and technical indicator interaction model
from patsy import dmatrix
minute_lag = 30
df = returns.loc[returns.index[30::10]].rename(columns={k: "asset_" + str(k) + "_return" for k in range(10)})
df = pd.concat([
    df,
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].max(0) for idx in df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_interval_high" for k in range(10)}).set_index(df.index),
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].min(0) for idx in df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_interval_low" for k in range(10)}).set_index(df.index),
    pd.concat([
        lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(x, 0)).sum(0) /
        (lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(x, 0)).sum(0) +
         lp.loc[idx - timedelta(minutes=minute_lag):idx].applymap(lambda x: max(-x, 0)).sum(0))
        for idx in df.index], axis=1).T.rename(
        columns={k: "asset_" + str(k) + "_rsi" for k in range(10)}).set_index(df.index)],
    axis=1)
for asset in np.arange(10):
    range_col = "asset_" + str(asset) + "_rel_price_range"
    volatility_col = "asset_" + str(asset) + "_range_volatility"
    interval_high_col = "asset_" + str(asset) + "_interval_high"
    interval_low_col = "asset_" + str(asset) + "_interval_low"
    df[range_col] = 2 * (df[interval_high_col] - df[interval_low_col]) / (df[interval_high_col] + df[interval_low_col])
    df[volatility_col] = np.sqrt(
        np.square(np.log(np.exp(df[interval_high_col]) / np.exp(df[interval_low_col]))) / (4 * np.log(2))
    )
df = df.reset_index()

train_df = pd.melt(df, id_vars = [c for c in df if "return" not in c],
                   value_vars=["asset_" + str(k) + "_return" for k in range(10)])
train_df['asset'] = train_df['variable'].str.replace("asset_", "").str.replace("_return", "")
train_df['cluster'] = 0
train_df.loc[train_df['asset'].isin(["0", "9", "8"]), "cluster"] = 1
train_df.loc[train_df['asset'] == "2", "cluster"] = 2
train_df = train_df.sort_values("timestamp").reset_index(drop = True)
train_df = (train_df.set_index(train_df["timestamp"])
            .drop(columns=['variable', "timestamp"])
            .rename(columns={'value': 'return'}))
spline_formula = " + ".join(["C(asset) * bs(%s, df=4)" % j for j in train_df if j not in ["return", "asset", "cluster"]])
spline_train_df = dmatrix("~ %s" % spline_formula, train_df, return_type="dataframe")
spline_train_df["return"] = train_df["return"]

WF_TRAIN_WINDOW = 100800 # 7 days * 1440 seconds * 10 assets
WF_TEST_WINDOW = 14400 # 1 day * 1440 seconds * 10 assets

# individual asset spline regression
spline_formula = " + ".join(["bs(%s, df=4)" % j for j in train_df if j not in ["return", "asset", "cluster"]])
spline_train_df = dmatrix("~ %s" % spline_formula, train_df[train_df["asset"] == "0"], return_type="dataframe")
spline_train_df["return"] = train_df.loc[train_df["asset"] == "0", "return"]
INDIVID_ASSET_WF_TRAIN_WINDOW = 7 * 1440 # 7 days * 1440 seconds
INDIVID_ASSET_WF_TEST_WINDOW = 10000 # 1 day * 1440 seconds
rf_spline_scores = walkforward_cv(spline_train_df, "return", [c for c in spline_train_df if "return" not in c],
                                  INDIVID_ASSET_WF_TRAIN_WINDOW, INDIVID_ASSET_WF_TEST_WINDOW, RandomForestRegressor,
                                  {"n_estimators": 50, "max_depth": 10, "max_features": 1.0,}, parallel=True)

def fit_model(data, y, regressor_cols, model_class, model_args):
    mod = model_class(**model_args)
    mod.fit(data[regressor_cols], data[y])
    return mod

individ_models = {}
for asset in range(10):
    spline_train_df = dmatrix("~ %s" % spline_formula, train_df[train_df["asset"] == str(asset)],
                              return_type="dataframe")
    spline_train_df["return"] = train_df.loc[train_df["asset"] == str(asset), "return"]
    individ_models['mod_' + str(asset)] = fit_model(
        spline_train_df, "return", [c for c in spline_train_df if "return" not in c],
        RandomForestRegressor,{"n_estimators": 50, "max_depth": 10, "max_features": 1.0,})

for mod_name, mod in individ_models.items():
    with open("../project/%s.pkl" % mod_name, "wb") as f:
        pickle.dump(mod, f)

analyze_idx = lp.index[::1440]
analyze_df = pd.concat([pd.DataFrame({
        "idx": idx,
        "asset": np.arange(10),
        "interval_high": lp.loc[idx - timedelta(minutes=1439):idx].max(0),
        "interval_low": lp.loc[idx - timedelta(minutes=1439):idx].min(0),
        "rsi": lp.loc[idx - timedelta(minutes=1439):idx].applymap(lambda x: max(x, 0)).sum(0) /
        (lp.loc[idx - timedelta(minutes=1439):idx].applymap(lambda x: max(x, 0)).sum(0) +
         lp.loc[idx - timedelta(minutes=1439):idx].applymap(lambda x: max(-x, 0)).sum(0)),
         }) for idx in analyze_idx], axis=0)

analyze_df = analyze_df.set_index(analyze_df["idx"]).drop(columns=["idx"])
kmeans = KMeans()
kmeans.fit(analyze_df[[c for c in analyze_df if c != "asset"]])
pd.DataFrame({'asset': analyze_df['asset'], 'label': kmeans.labels_})

import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.cluster import KMeans
fig, ax = plt.subplots(1, 1, figsize=(12,12))
cmap = dict(zip(np.arange(10), ["#32a852", "#73149c", "#f037c5", "#f09a37", "#1324d6",
                              "#fa141f", "#0ee3df", "#69adfa", "#e79cf0", "#120214"]))
metric = "interval_high"
for asset in np.arange(10):
    plt.plot(analyze_df[analyze_df["asset"] == asset].index,
             analyze_df.loc[analyze_df["asset"] == asset, metric],
             color=cmap[asset], label="asset_" + str(asset))
plt.legend()
plt.show()


pred_df = pd.DataFrame({
    "asset": np.arange(10),
    "weekday_"  + str(pred_idx.day_of_week): np.ones(10),
    "log_vol_sum": np.log(B.loc[pred_idx - timedelta(minutes=MOD.minute_lag):pred_idx].sum(0)),
    "interval_high": A.loc[pred_idx - timedelta(minutes=MOD.minute_lag):pred_idx].max(0),
    "interval_low": A.loc[pred_idx - timedelta(minutes=MOD.minute_lag):pred_idx].min(0),
    "rsi": A.loc[pred_idx - timedelta(minutes=MOD.rsi_k):pred_idx].applymap(lambda x: max(x, 0)).sum(0) /
    (A.loc[pred_idx - timedelta(minutes=MOD.rsi_k):pred_idx].applymap(lambda x: max(x, 0)).sum(0) +
     A.loc[pred_idx - timedelta(minutes=MOD.rsi_k):pred_idx].applymap(lambda x: max(-x, 0)).sum(0)),
})
